{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import hashlib\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, average_precision_score, fbeta_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_text = 'reviewText'\n",
    "all_keys = ['reviewText', 'reviewerID', 'reviewTime', 'asin', 'reviewerName', 'overall', 'unixReviewTime', 'summary', 'helpful']\n",
    "req_keys = ['asin', 'reviewText', 'overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordlist = {}\n",
    "wordindex = {}\n",
    "freq_all = []\n",
    "freq = [[] for i in range(5)]\n",
    "i_star = [0, 0, 0, 0, 0]\n",
    "\n",
    "phraselist = {}\n",
    "phraseindex = {}\n",
    "freq_all_p = []\n",
    "freq_p = [[] for i in range(5)]\n",
    "\n",
    "sentiment_pos = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "negation_words = ['not', 'Not']\n",
    "threshold = 10\n",
    "index_above_threshold = {}\n",
    "sentiment_score = []\n",
    "index_above_threshold_p = {}\n",
    "sentiment_score_p = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preInitialize():\n",
    "    global wordlist, wordindex, phraselist, phraseindex\n",
    "    \n",
    "    wordlist = {}\n",
    "    wordindex = {}\n",
    "    phraselist = {}\n",
    "    phraseindex = {}\n",
    "\n",
    "def initialize():\n",
    "    global freq_all, freq, i_star, sentiment_score, threshold, index_above_threshold \n",
    "    global freq_all_p, freq_p, index_above_threshold_p, sentiment_score_p\n",
    "\n",
    "    l_w = len(wordlist)\n",
    "    freq_all = [0]*l_w\n",
    "    freq = [[0]*l_w for i in range(5)]\n",
    "\n",
    "    l_p = len(phraselist)\n",
    "    freq_all_p = [0]*l_p\n",
    "    freq_p = [[0]*l_p for i in range(5)]\n",
    "\n",
    "    i_star = [0, 0, 0, 0, 0]\n",
    "\n",
    "    sentiment_score = []\n",
    "    sentiment_score_p = []\n",
    "    threshold = 10\n",
    "    index_above_threshold = {}\n",
    "    index_above_threshold_p = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(path): \n",
    "    g = gzip.open(path, 'rb') \n",
    "    for l in g:\n",
    "        yield eval(l) \n",
    "\n",
    "def count_tokens(tokens, overall):\n",
    "    i_star[overall-1]+=1\n",
    "\n",
    "    for token, pos in tokens:\n",
    "        try:\n",
    "            index = wordlist[token]\n",
    "            freq_all[index] += 1\n",
    "            freq[overall-1][index] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def count_phrase(phrases, overall):\n",
    "    for token, pos in phrases:\n",
    "        index = phraselist[token]\n",
    "        freq_all_p[index] += 1\n",
    "        freq_p[overall-1][index] += 1\n",
    "\n",
    "def phrase_tagger(tokens):\n",
    "    neg_phrase = []\n",
    "    j = 0\n",
    "    i = 0\n",
    "    l = len(tokens)\n",
    "    while i < (l - 2):\n",
    "        if tokens[i][0] in negation_words:\n",
    "            if tokens[i + 1][1] in adj:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0], 'NOA'))\n",
    "                i+=1\n",
    "            elif tokens[i+1][1] in verb:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0], 'NOV'))\n",
    "                i+=1\n",
    "            elif tokens[i+2][1] in adj:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0] + ' ' + tokens[i + 2][0], 'NOA'))\n",
    "                i+=2\n",
    "            elif tokens[i+2][1] in verb:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0] + ' ' + tokens[i + 2][0], 'NOV'))\n",
    "                i+=2\n",
    "        i+=1\n",
    "    return neg_phrase\n",
    "\n",
    "def pos_tagger(sentence):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "def initializeWordPhraselist(tokens, phrases, cntr1, cntr2):\n",
    "    for token, pos in tokens:\n",
    "        if pos in sentiment_pos:\n",
    "            try:\n",
    "                index = wordlist[token]\n",
    "            except:\n",
    "                wordlist[token] = cntr1\n",
    "                wordindex[cntr1] = token\n",
    "                cntr1+=1\n",
    "                \n",
    "    for token, pos in phrases:\n",
    "        try:\n",
    "            index = phraselist[token]\n",
    "        except:\n",
    "            phraselist[token] = cntr2\n",
    "            phraseindex[cntr2] = token\n",
    "            cntr2+=1\n",
    "        \n",
    "    return cntr1, cntr2\n",
    "\n",
    "def getDF_all(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    cntr1 = 0\n",
    "    cntr2 = 0\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        df[i][review_text] = pos_tagger(df[i][review_text])\n",
    "        df[i]['phraseTokens'] = phrase_tagger(df[i][review_text])\n",
    "        cntr1, cntr2 = initializeWordPhraselist(df[i][review_text], df[i]['phraseTokens'], cntr1, cntr2)\n",
    "        i+=1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    cntr1 = 0\n",
    "    cntr2 = 0\n",
    "    for d in parse(path):\n",
    "        df[i] = {key: d[key] for key in req_keys}\n",
    "        # print(cntr, df[i]['overall'])\n",
    "        df[i][review_text] = pos_tagger(df[i][review_text])\n",
    "        df[i]['phraseTokens'] = phrase_tagger(df[i][review_text])\n",
    "        cntr1, cntr2 = initializeWordPhraselist(df[i][review_text], df[i]['phraseTokens'], cntr1, cntr2)\n",
    "        i += 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16399\n",
      "1666\n"
     ]
    }
   ],
   "source": [
    "SOURCES = ['data/reviews_Musical_Instruments_5.json.gz']\n",
    "\n",
    "preInitialize()\n",
    "df = pd.DataFrame()\n",
    "#df = pd.DataFrame({'reviewText': [], 'reviewerID': [], 'reviewTime': [], 'asin': [], 'reviewerName': [], 'overall': [], 'unixReviewTime': [], 'summary': [], 'helpful': []})\n",
    "for path in SOURCES:\n",
    "    df = df.append(getDF(path))\n",
    "\n",
    "print(len(wordlist))\n",
    "print(len(phraselist))\n",
    "\n",
    "initialize()\n",
    "for index, review in df.iterrows():\n",
    "    count_tokens(review[review_text], int(review['overall']))\n",
    "    count_phrase(review['phraseTokens'], int(review['overall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1666"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phraselist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16399"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_freq_all = np.array(freq_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16399,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_freq_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_freqs = np.array([freq[0], freq[1], freq[2], freq[3], freq[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16399)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_freq_start(i_star):\n",
    "    plt.xticks(range(5), ['1-star', '2-star', '3-star', '4-star', '5-star'])\n",
    "    plt.yticks()\n",
    "    plt.bar(range(5), i_star, align='center', color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f786b786cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_freq_start(i_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentimentScore():\n",
    "    cntr = 0\n",
    "    for word, index in wordlist.items():\n",
    "        if freq_all[index] > threshold:\n",
    "            num = 0\n",
    "            den = 0\n",
    "            for i in range(5):\n",
    "                temp = (i_star[4]/i_star[i]) * freq[i][index]\n",
    "                num += (i+1) * temp\n",
    "                den += temp\n",
    "            #print(num/den)\n",
    "            sentiment_score.append((num/den))\n",
    "            index_above_threshold[index] = cntr\n",
    "            cntr += 1\n",
    "\n",
    "    cntr = 0\n",
    "    for word, index in phraselist.items():\n",
    "        if freq_all_p[index] > threshold:\n",
    "            num = 0\n",
    "            den = 0\n",
    "            for i in range(5):\n",
    "                temp = (i_star[4]/i_star[i]) * freq_p[i][index]\n",
    "                num += (i+1) * temp\n",
    "                den += temp\n",
    "            #print(num/den)\n",
    "            sentiment_score_p.append((num/den))\n",
    "            index_above_threshold_p[index] = cntr\n",
    "            cntr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentimentScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3230"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sen_tokens = len(sentiment_score)\n",
    "len_sen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_score_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_vector_s(sentence):\n",
    "    pos_tokens = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    phrase_tokens = phrase_tagger(pos_tokens)\n",
    "    return f_vector_p(pos_tokens, phrase_tokens)\n",
    "\n",
    "def f_vector_p(pos_tokens, phrase_tokens):\n",
    "    f1 = [ 0 ]*len(sentiment_score)\n",
    "    total_score = 0.0\n",
    "    num_tokens = 0.0\n",
    "    for token, pos in pos_tokens:\n",
    "        if pos in sentiment_pos:\n",
    "            try:\n",
    "                index = wordlist[token]\n",
    "                cntr = index_above_threshold[index]\n",
    "                f1[cntr] = 1\n",
    "                total_score += sentiment_score[cntr]\n",
    "                num_tokens = num_tokens + 1.0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if num_tokens == 0:\n",
    "        avg_score = 0\n",
    "    else:\n",
    "        avg_score = total_score/num_tokens\n",
    "\n",
    "    f1.append(avg_score)\n",
    "\n",
    "    f2 = [ 0 ]*len(sentiment_score_p)\n",
    "    total_score = 0.0\n",
    "    num_tokens = 0.0\n",
    "    for token, pos in phrase_tokens:\n",
    "        try:\n",
    "            index = phraselist[token]\n",
    "            cntr = index_above_threshold_p[index]\n",
    "            f2[cntr] = 1\n",
    "            total_score += sentiment_score_p[cntr]\n",
    "            num_tokens = num_tokens + 1.0\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if num_tokens == 0:\n",
    "        avg_score = 0\n",
    "    else:\n",
    "        avg_score = total_score/num_tokens\n",
    "\n",
    "    f1 = f1 + f2\n",
    "    f1.append(avg_score)\n",
    "    #return [hashlib.md5(''.join(f1).encode()).hexdigest(), avg_score]\n",
    "    #return [hash(''.join(f1)), avg_score]\n",
    "    return np.array(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preInitialize()\n",
    "data = pd.DataFrame()\n",
    "for file in SOURCES:\n",
    "    data = data.append(getDF_all(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10261"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vectorize():\n",
    "    def fit(self, data):\n",
    "        # print(len(data))\n",
    "        initialize()\n",
    "        # self.pos_s = []\n",
    "        cntr = 0\n",
    "        cntr2 = 0\n",
    "        for index, review in data.iterrows():\n",
    "            #print(review)\n",
    "            count_tokens(review['reviewText'], int(review['overall']))\n",
    "            count_phrase(review['phraseTokens'], int(review['overall']))\n",
    "            # self.pos_s.append(pos)\n",
    "        sentimentScore()\n",
    "\n",
    "    def transform(self, data):\n",
    "        feature_vector_list = []\n",
    "        for index, review in data.iterrows():\n",
    "            feature_vector_list.append(f_vector_p(review['reviewText'], review['phraseTokens']))\n",
    "        return np.array(feature_vector_list)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        # print(len(data))\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#phraselist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10261\n",
      "10261\n"
     ]
    }
   ],
   "source": [
    "vectorize = Vectorize()\n",
    "feature_vector = vectorize.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10261, 3289)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "targets = data['overall'].values\n",
    "classifier.fit(feature_vector, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  5.,  5., ...,  4.,  4.,  4.])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(f_vector_s('Not much to write about here, but it does exactly what it\\'s supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,').reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kFoldTest(classifier):\n",
    "    pipeline = Pipeline([\n",
    "            ('vectorizer', Vectorize()),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ])\n",
    "\n",
    "    kFold = KFold(n = len(data), n_folds = 4)\n",
    "    scores = []\n",
    "    confusionMatrix = np.array([[0]*5]*5)\n",
    "    vectorizer = Vectorize()\n",
    "    #classifier = MultinomialNB()\n",
    "    #classifier = BernoulliNB()\n",
    "    #classifier = SVC()\n",
    "    #classifier = RandomForestClassifier()\n",
    "    for train_indices, test_indices in kFold:\n",
    "        train_data = data.iloc[train_indices]\n",
    "        train_y = data.iloc[train_indices]['overall'].values\n",
    "\n",
    "        test_data = data.iloc[test_indices]\n",
    "        test_y = data.iloc[test_indices]['overall'].values\n",
    "\n",
    "        fs = vectorizer.fit_transform(train_data)\n",
    "        classifier.fit(fs, train_y)\n",
    "        # print(fs.shape)\n",
    "        fs = vectorizer.transform(test_data)\n",
    "        # print(fs.shape)\n",
    "        predictions = classifier.predict(fs)\n",
    "\n",
    "        # print(test_y.shape)\n",
    "        # print(predictions.shape)\n",
    "        # print(confusion_matrix(test_y, predictions))\n",
    "        confusionMatrix += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label = 5, average='weighted')\n",
    "        scores.append(score)\n",
    "\n",
    "    print('Total movies classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total movies classified: 10261\n",
      "Score: 0.585194914767\n",
      "Confusion matrix:\n",
      "[[  11    3   17   22  164]\n",
      " [   5    4   13   36  192]\n",
      " [   6    5   37  121  603]\n",
      " [  12    1   38  281 1752]\n",
      " [   7    6   57  473 6395]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total movies classified: 10261\n",
      "Score: 0.607302008853\n",
      "Confusion matrix:\n",
      "[[  15    6   32   33  131]\n",
      " [   2    2   38   59  149]\n",
      " [   1    2   68  202  499]\n",
      " [   1    1   67  421 1594]\n",
      " [   5    3   95  562 6273]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total movies classified: 10261\n",
      "Score: 0.545560039584\n",
      "Confusion matrix:\n",
      "[[   0    0    0    0  217]\n",
      " [   0    0    0    0  250]\n",
      " [   0    0    0    0  772]\n",
      " [   0    0    0    0 2084]\n",
      " [   0    0    0    0 6938]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total movies classified: 10261\n",
      "Score: 0.577275954265\n",
      "Confusion matrix:\n",
      "[[   7   13   35   27  135]\n",
      " [   6   17   37   38  152]\n",
      " [   5   51   91  152  473]\n",
      " [  15  106  158  377 1428]\n",
      " [  27  249  294  666 5702]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
