{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import hashlib\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, average_precision_score, fbeta_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_text = 'reviewText'\n",
    "all_keys = ['reviewText', 'reviewerID', 'reviewTime', 'asin', 'reviewerName', 'overall', 'unixReviewTime', 'summary', 'helpful']\n",
    "req_keys = ['asin', 'reviewText', 'overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordlist = {}\n",
    "wordindex = {}\n",
    "freq_all = []\n",
    "freq = [[] for i in range(5)]\n",
    "i_star = [0, 0, 0, 0, 0]\n",
    "\n",
    "phraselist = {}\n",
    "phraseindex = {}\n",
    "freq_all_p = []\n",
    "freq_p = [[] for i in range(5)]\n",
    "\n",
    "sentiment_pos = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "negation_words = ['not', 'Not']\n",
    "threshold = 10\n",
    "index_above_threshold = {}\n",
    "sentiment_score = []\n",
    "index_above_threshold_p = {}\n",
    "sentiment_score_p = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preInitialize():\n",
    "    global wordlist, wordindex, phraselist, phraseindex\n",
    "    \n",
    "    wordlist = {}\n",
    "    wordindex = {}\n",
    "    phraselist = {}\n",
    "    phraseindex = {}\n",
    "\n",
    "def initialize():\n",
    "    global freq_all, freq, i_star, sentiment_score, threshold, index_above_threshold \n",
    "    global freq_all_p, freq_p, index_above_threshold_p, sentiment_score_p\n",
    "\n",
    "    l_w = len(wordlist)\n",
    "    freq_all = [0]*l_w\n",
    "    freq = [[0]*l_w for i in range(5)]\n",
    "\n",
    "    l_p = len(phraselist)\n",
    "    freq_all_p = [0]*l_p\n",
    "    freq_p = [[0]*l_p for i in range(5)]\n",
    "\n",
    "    i_star = [0, 0, 0, 0, 0]\n",
    "\n",
    "    sentiment_score = []\n",
    "    sentiment_score_p = []\n",
    "    threshold = 10\n",
    "    index_above_threshold = {}\n",
    "    index_above_threshold_p = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing zip; Count tokens, phrases; POS tagging, Phrase tagging, Building DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(path): \n",
    "    g = gzip.open(path, 'rb') \n",
    "    for l in g:\n",
    "        yield eval(l) \n",
    "\n",
    "def count_tokens(tokens, overall):\n",
    "    i_star[overall-1]+=1\n",
    "\n",
    "    for token, pos in tokens:\n",
    "        try:\n",
    "            index = wordlist[token]\n",
    "            freq_all[index] += 1\n",
    "            freq[overall-1][index] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def count_phrase(phrases, overall):\n",
    "    for token, pos in phrases:\n",
    "        index = phraselist[token]\n",
    "        freq_all_p[index] += 1\n",
    "        freq_p[overall-1][index] += 1\n",
    "\n",
    "def phrase_tagger(tokens):\n",
    "    neg_phrase = []\n",
    "    j = 0\n",
    "    i = 0\n",
    "    l = len(tokens)\n",
    "    while i < (l - 2):\n",
    "        if tokens[i][0] in negation_words:\n",
    "            if tokens[i + 1][1] in adj:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0], 'NOA'))\n",
    "                i+=1\n",
    "            elif tokens[i+1][1] in verb:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0], 'NOV'))\n",
    "                i+=1\n",
    "            elif tokens[i+2][1] in adj:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0] + ' ' + tokens[i + 2][0], 'NOA'))\n",
    "                i+=2\n",
    "            elif tokens[i+2][1] in verb:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0] + ' ' + tokens[i + 2][0], 'NOV'))\n",
    "                i+=2\n",
    "        i+=1\n",
    "    return neg_phrase\n",
    "\n",
    "def pos_tagger(sentence):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "def initializeWordPhraselist(tokens, phrases, cntr1, cntr2):\n",
    "    for token, pos in tokens:\n",
    "        if pos in sentiment_pos:\n",
    "            try:\n",
    "                index = wordlist[token]\n",
    "            except:\n",
    "                wordlist[token] = cntr1\n",
    "                wordindex[cntr1] = token\n",
    "                cntr1+=1\n",
    "                \n",
    "    for token, pos in phrases:\n",
    "        try:\n",
    "            index = phraselist[token]\n",
    "        except:\n",
    "            phraselist[token] = cntr2\n",
    "            phraseindex[cntr2] = token\n",
    "            cntr2+=1\n",
    "        \n",
    "    return cntr1, cntr2\n",
    "\n",
    "def getDF_all(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    cntr1 = 0\n",
    "    cntr2 = 0\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        df[i][review_text] = pos_tagger(df[i][review_text])\n",
    "        df[i]['phraseTokens'] = phrase_tagger(df[i][review_text])\n",
    "        cntr1, cntr2 = initializeWordPhraselist(df[i][review_text], df[i]['phraseTokens'], cntr1, cntr2)\n",
    "        i+=1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    cntr1 = 0\n",
    "    cntr2 = 0\n",
    "    for d in parse(path):\n",
    "        df[i] = {key: d[key] for key in req_keys}\n",
    "        # print(cntr, df[i]['overall'])\n",
    "        df[i][review_text] = pos_tagger(df[i][review_text])\n",
    "        df[i]['phraseTokens'] = phrase_tagger(df[i][review_text])\n",
    "        cntr1, cntr2 = initializeWordPhraselist(df[i][review_text], df[i]['phraseTokens'], cntr1, cntr2)\n",
    "        i += 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample on how to use above methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16399\n",
      "1666\n"
     ]
    }
   ],
   "source": [
    "SOURCES = ['/home/hduser/Documents/sentiment_speech/data/reviews_Musical_Instruments_5.json.gz']\n",
    "\n",
    "preInitialize()\n",
    "df = pd.DataFrame()\n",
    "#df = pd.DataFrame({'reviewText': [], 'reviewerID': [], 'reviewTime': [], 'asin': [], 'reviewerName': [], 'overall': [], 'unixReviewTime': [], 'summary': [], 'helpful': []})\n",
    "for path in SOURCES:\n",
    "    df = df.append(getDF(path))\n",
    "\n",
    "print(len(wordlist))\n",
    "print(len(phraselist))\n",
    "\n",
    "initialize()\n",
    "for index, review in df.iterrows():\n",
    "    count_tokens(review[review_text], int(review['overall']))\n",
    "    count_phrase(review['phraseTokens'], int(review['overall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phraselist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16399"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_freq_all = np.array(freq_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16399,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_freq_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_freqs = np.array([freq[0], freq[1], freq[2], freq[3], freq[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16399)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_freq_start(i_star):\n",
    "    plt.xticks(range(5), ['1-star', '2-star', '3-star', '4-star', '5-star'])\n",
    "    plt.yticks()\n",
    "    plt.bar(range(5), i_star, align='center', color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution according to rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE9RJREFUeJzt3X+s3fV93/Hny3EJoSEMtcXObLa4oqaQZQ2u5myiU06U\nxIRUslG1MbJqwGCrNJwRbd1UjCZxI1Ui2f4okSbQtqbBREmJ0ym1N1ngMFKkVGrsAKlR7IC1+FL7\nKr60S+WURdpgvPfH+V7z5XJv7rn4nvvj4+dD+up+z/t8Pud8vh9fv+7Xn+/3XKeqkCS1a91KD0CS\nNF4GvSQ1zqCXpMYZ9JLUOINekhpn0EtS4xYM+iRbkzyb5Jnu69kkdye5PMmhJM8neTzJZb0+e5Kc\nSHI8yY5efVuSo0leSPLAuA5KkvS6LOY++iTrgNPAB4BPAv+rqv59kt8CLq+qe5JcC3wJ+DvAZuAJ\n4BeqqpJ8C/hkVR1JchD4XFU9vsTHJEnqWezSzUeA/1lVp4BdwN6uvhe4qdvfCTxaVa9W1SRwAtie\nZCNwaVUd6do90usjSRqTxQb9PwK+3O1vqKppgKo6A1zR1TcBp3p9prraJob/GphxuqtJksZo5KBP\n8lMMz9a/2pVmr/n4uxQkaRVav4i2NwJPV9VfdI+nk2yoquluWealrj4FXNnrt7mrzVd/kyT+0JCk\nt6CqMru2mKWbTwC/33t8ALi9278N2N+r35LkoiRbgKuAw93yztkk25MEuLXXZ67BLtt23333Lev7\nrYXNOXFenJe1Ny/zGemMPsklDC/E/kav/FlgX5I7gBeBm7uAPpZkH3AMeAW4q14fwW7gYeBi4GBV\nPTbK+0vSUti4eSPTU9PL+p6f/vSnl/X95jJS0FfVj4Gfm1X7IcPwn6v9/cD9c9SfBt63+GFK0vmb\nnpqGiWV8w28AH1rG95uYu+wnY4HBYLDSQ1h1nJO5OS9zc17m8Z6VHsDQoj4wtVyS1Gocl6S1Lcny\nntEvt4nzvxgrSVqDDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9J\njTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDVupKBPclmSryY5\nnuS7ST6Q5PIkh5I8n+TxJJf12u9JcqJrv6NX35bkaJIXkjwwjgOSJL3RqGf0nwMOVtU1wC8B3wPu\nAZ6oqquBJ4E9AEmuBW4GrgFuBB5Mku51HgLurKqtwNYkNyzZkUiS5rRg0Cd5F/D3q+oLAFX1alWd\nBXYBe7tme4Gbuv2dwKNdu0ngBLA9yUbg0qo60rV7pNdHkjQmo5zRbwH+IskXkjyT5D8nuQTYUFXT\nAFV1Briia78JONXrP9XVNgGne/XTXU2SNEbrR2yzDdhdVd9O8jsMl21qVrvZj8/LxMTEuf3BYMBg\nMFjKl5ekte8kMLlws1GC/jRwqqq+3T3+rwyDfjrJhqqa7pZlXuqenwKu7PXf3NXmq8+pH/SSpDls\n6bYZT83dbMGlm2555lSSrV3pw8B3gQPA7V3tNmB/t38AuCXJRUm2AFcBh7vlnbNJtncXZ2/t9ZEk\njckoZ/QAdwNfSvJTwPeBfwq8DdiX5A7gRYZ32lBVx5LsA44BrwB3VdXMss5u4GHgYoZ38Ty2VAci\nSZpbXs/g1SNJrcZxSVrbksDESo9ijCagqjK77CdjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMM\neklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCX\npMYZ9JLUOINekhpn0EtS4wx6SWrcSEGfZDLJnyZ5NsnhrnZ5kkNJnk/yeJLLeu33JDmR5HiSHb36\ntiRHk7yQ5IGlPxxJ0myjntG/Bgyq6rqq2t7V7gGeqKqrgSeBPQBJrgVuBq4BbgQeTJKuz0PAnVW1\nFdia5IYlOg5J0jxGDfrM0XYXsLfb3wvc1O3vBB6tqlerahI4AWxPshG4tKqOdO0e6fWRJI3JqEFf\nwNeTHEnyz7rahqqaBqiqM8AVXX0TcKrXd6qrbQJO9+qnu5okaYzWj9ju+qr6QZKfAw4leZ5h+PfN\nfnxeJiYmzu0PBgMGg8FSvrwkrX0ngcmFm40U9FX1g+7rnyf5Q2A7MJ1kQ1VNd8syL3XNp4Are903\nd7X56nPqB70kaQ5bum3GU3M3W3DpJsklSd7Z7f80sAN4DjgA3N41uw3Y3+0fAG5JclGSLcBVwOFu\needsku3dxdlbe30kSWMyyhn9BuBrSapr/6WqOpTk28C+JHcALzK804aqOpZkH3AMeAW4q6pmlnV2\nAw8DFwMHq+qxJT0aSdKb5PUMXj2S1Gocl6S1LQlMrPQoxmgCqiqzy34yVpIaZ9BLUuMMeklqnEEv\nSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLU\nOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGjRz0SdYleSbJge7x5UkOJXk+yeNJLuu1\n3ZPkRJLjSXb06tuSHE3yQpIHlvZQJElzWcwZ/aeAY73H9wBPVNXVwJPAHoAk1wI3A9cANwIPJknX\n5yHgzqraCmxNcsN5jl+StICRgj7JZuDjwO/2yruAvd3+XuCmbn8n8GhVvVpVk8AJYHuSjcClVXWk\na/dIr48kaUxGPaP/HeDfAtWrbaiqaYCqOgNc0dU3Aad67aa62ibgdK9+uqtJksZo/UINkvwqMF1V\n30ky+AlN6yc8t2gTExPn9geDAYPBT3prSboAnQQmF262YNAD1wM7k3wceAdwaZIvAmeSbKiq6W5Z\n5qWu/RRwZa//5q42X31O/aCXJM1hS7fNeGruZgsu3VTVvVX1N6rq54FbgCer6p8A/w24vWt2G7C/\n2z8A3JLkoiRbgKuAw93yztkk27uLs7f2+kiSxmSUM/r5fAbYl+QO4EWGd9pQVceS7GN4h84rwF1V\nNbOssxt4GLgYOFhVj53H+0uSRpDXM3j1SFKrcVyS1rYkMLHSoxijCaiqzC77yVhJapxBL0mNM+gl\nqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa\nZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGrdg0Cd5e5JvJXk2yXNJ7uvqlyc5\nlOT5JI8nuazXZ0+SE0mOJ9nRq29LcjTJC0keGM8hSZL6Fgz6qvo/wIeq6jrg/cCNSbYD9wBPVNXV\nwJPAHoAk1wI3A9cANwIPJkn3cg8Bd1bVVmBrkhuW+oAkSW800tJNVf242307sB4oYBewt6vvBW7q\n9ncCj1bVq1U1CZwAtifZCFxaVUe6do/0+kiSxmSkoE+yLsmzwBng611Yb6iqaYCqOgNc0TXfBJzq\ndZ/qapuA07366a4mSRqj9aM0qqrXgOuSvAv4WpL3Mjyrf0OzpRzYxMTEuf3BYMBgMFjKl5ekte8k\nMLlws5GCfkZV/SjJHwEfA6aTbKiq6W5Z5qWu2RRwZa/b5q42X31O/aCXJM1hS7fNeGruZqPcdfOz\nM3fUJHkH8FHgOHAAuL1rdhuwv9s/ANyS5KIkW4CrgMPd8s7ZJNu7i7O39vpIksZklDP6dwN7k6xj\n+IPhK1V1MMmfAPuS3AG8yPBOG6rqWJJ9wDHgFeCuqppZ1tkNPAxcDBysqseW9GgkSW+S1zN49UhS\nq3Fckta2JDCx0qMYowmoqswu+8lYSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMM\neklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCX\npMYZ9JLUOINekhq3YNAn2ZzkySTfTfJckru7+uVJDiV5PsnjSS7r9dmT5ESS40l29OrbkhxN8kKS\nB8ZzSJKkvlHO6F8F/nVVvRf4e8DuJL8I3AM8UVVXA08CewCSXAvcDFwD3Ag8mCTdaz0E3FlVW4Gt\nSW5Y0qORBMDGzRtJ0uy2cfPGlZ7iNWX9Qg2q6gxwptt/OclxYDOwC/hg12wv8EcMw38n8GhVvQpM\nJjkBbE/yInBpVR3p+jwC3AQ8vnSHIwlgemoaJlZ6FOMzPTG90kNYUxa1Rp/kPcD7gT8BNlTVNJz7\nYXBF12wTcKrXbaqrbQJO9+qnu5okaYwWPKOfkeSdwB8An+rO7GtWk9mPz8vExMS5/cFgwGAwWMqX\nl6S17yQwuXCzkYI+yXqGIf/FqtrflaeTbKiq6SQbgZe6+hRwZa/75q42X31O/aCXJM1hS7fNeGru\nZqMu3fwecKyqPterHQBu7/ZvA/b36rckuSjJFuAq4HC3vHM2yfbu4uytvT6SpDFZ8Iw+yfXArwPP\nJXmW4RLNvcBngX1J7gBeZHinDVV1LMk+4BjwCnBXVc0s6+wGHgYuBg5W1WNLeziSpNlGuevmj4G3\nzfP0R+bpcz9w/xz1p4H3LWaAkqTz4ydjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEv\nSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLU\nOINekhpn0EtS4wx6SWrcgkGf5PNJppMc7dUuT3IoyfNJHk9yWe+5PUlOJDmeZEevvi3J0SQvJHlg\n6Q9FkjSXUc7ovwDcMKt2D/BEVV0NPAnsAUhyLXAzcA1wI/BgknR9HgLurKqtwNYks19TkjQGCwZ9\nVX0T+MtZ5V3A3m5/L3BTt78TeLSqXq2qSeAEsD3JRuDSqjrStXuk10eSNEZvdY3+iqqaBqiqM8AV\nXX0TcKrXbqqrbQJO9+qnu5okaczWL9Hr1BK9zjkTExPn9geDAYPBYKnfQpLWtpPA5MLN3mrQTyfZ\nUFXT3bLMS119Criy125zV5uvPq9+0EuS5rCl22Y8NXezUZdu0m0zDgC3d/u3Aft79VuSXJRkC3AV\ncLhb3jmbZHt3cfbWXh9J0hgteEaf5MvAAPiZJH8G3Ad8BvhqkjuAFxneaUNVHUuyDzgGvALcVVUz\nyzq7gYeBi4GDVfXY0h6KJGkuCwZ9Vf3jeZ76yDzt7wfun6P+NPC+RY1OknTe/GSsJDXOoJekxi3V\n7ZXSiti4eSPTU9MrPYyx2bBpA2dOn1npYWiNM+i1pk1PTcPESo9ifKYn2v0hpuXj0o0kNc6gl6TG\nGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxB\nL0mN8/fRrxH+BxuS3qpVG/RJVnoIY/NWQs3/YEPSW7Vqg95Qk6Slsexr9Ek+luR7SV5I8lvL/f6S\ndKFZ1qBPsg74j8ANwHuBTyT5xeUcw5xOrvQAViHnZG7Oy9ycl7mtknlZ7jP67cCJqnqxql4BHgV2\nLfMY3mxypQewCk2u9ABWqcmVHsAqNbnSA1ilJld6AEPLHfSbgFO9x6e7miRpTLyPXpIal6pavjdL\n/i4wUVUf6x7fA1RVfXZWu+UblCQ1pKredG/6cgf924DngQ8DPwAOA5+oquPLNghJusAs6330VfX/\nknwSOMRw2ejzhrwkjdeyntFLkpZfUxdjk3w+yXSSo4vs96kkF49rXCspyeYkTyb5bpLnkty9iL4t\nz8vbk3wrybPdvNy3iL7NzsuMJOuSPJPkwCL6ND0vSSaT/Gn3PXN4Ef1WfF6aOqNP8ivAy8AjVfW3\nF9HvJPDLVfXDRfRZV1WvvYVhLqskG4GNVfWdJO8EngZ2VdX3Rujb7LwAJLmkqn7cXTv6Y+Duqlrw\nL3Dr8wKQ5F8Bvwy8q6p2jtin6XlJ8n2Gx/eXi+y34vPS1Bl9VX0TmPcPIcklSf579xP5aJJ/mORf\nAn8d+EaS/9G1ezDJ4dlneklOJvlMkm8D/2Dcx7MUqupMVX2n238ZOM6szy5ciPMCUFU/7nbfzvB6\n1RvOei7UeUmyGfg48LvzPH9BzgsQfkJmrup5qaqmNuBvAkfnee7XgP/Ue3xp9/X7wOW9+l/rvq4D\nvgH8re7xSeDfrPQxnsfcvIfhZ/Xe6bycO45ngR8B9/v9cu54vgq8H/ggcMB5OXc83weeAY4A/3wt\nzUtTZ/QjeA74aJL7k/xKVf1VV0+3zbglydMMQ+DabpvxleUZ6tLqlm3+APhUDc/s+y7Ieamq16rq\nOmAz8IEk185qcsHNS5JfBaZr+K/A2cc544Kbl871VbWN4b92dndLxX2rdl6aDvoML0Q+211U+o2q\nOgFsY/gH8ttJ/t0cfd4D/Cbwoar6JeAg0L+Q8r/HP/KllWQ9w5D/YlXtd17eqKp+xPDM6tecF64H\ndnbr0b8PfCjJ150XqKofdF//HPgaMFgr87J6fx/9W3fup2dVnQauO/dE8m7gh1X15SRngTu7p34E\nvAv4Yff1ZeCvkmwAbmQYAmvZ7wHHqupz4LwAJPlZ4JWqOpvkHcBHgc9U1W/32lxw81JV9wL3AiT5\nIPCbNeti7IU4L0kuAdZV1ctJfhrYAXx6rXy/NBX0Sb4MDICfSfJnwH1V9YVek/cB/yHJa8D/Bf5F\nV/8vwGNJpqrqw0m+w/Ci5Sngm73+a+4WpSTXA78OPJfkWYbHcG9VPdZrdsHNC/BuYG+Gvzp7HfCV\nqjo4q82FOC+juBDnZQPwtQx/Pct64EtVdWhWm1U7L03dXilJerOm1+glSQa9JDXPoJekxhn0ktQ4\ng16SGmfQS1LjDHpJapxBL0mN+/+e4uOAEbOqSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7e64a6080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_freq_start(i_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Sentiment Score of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentimentScore():\n",
    "    cntr = 0\n",
    "    for word, index in wordlist.items():\n",
    "        if freq_all[index] > threshold:\n",
    "            num = 0\n",
    "            den = 0\n",
    "            for i in range(5):\n",
    "                temp = (i_star[4]/i_star[i]) * freq[i][index]\n",
    "                num += (i+1) * temp\n",
    "                den += temp\n",
    "            #print(num/den)\n",
    "            sentiment_score.append((num/den))\n",
    "            index_above_threshold[index] = cntr\n",
    "            cntr += 1\n",
    "\n",
    "    cntr = 0\n",
    "    for word, index in phraselist.items():\n",
    "        if freq_all_p[index] > threshold:\n",
    "            num = 0\n",
    "            den = 0\n",
    "            for i in range(5):\n",
    "                temp = (i_star[4]/i_star[i]) * freq_p[i][index]\n",
    "                num += (i+1) * temp\n",
    "                den += temp\n",
    "            #print(num/den)\n",
    "            sentiment_score_p.append((num/den))\n",
    "            index_above_threshold_p[index] = cntr\n",
    "            cntr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentimentScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3230"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sen_tokens = len(sentiment_score)\n",
    "len_sen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_score_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vectors given the sentence or pos tags depending upon existing token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_vector_s(sentence):\n",
    "    pos_tokens = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    phrase_tokens = phrase_tagger(pos_tokens)\n",
    "    return f_vector_p(pos_tokens, phrase_tokens)\n",
    "\n",
    "def f_vector_p(pos_tokens, phrase_tokens):\n",
    "    f1 = [ 0 ]*len(sentiment_score)\n",
    "    total_score = 0.0\n",
    "    num_tokens = 0.0\n",
    "    for token, pos in pos_tokens:\n",
    "        if pos in sentiment_pos:\n",
    "            try:\n",
    "                index = wordlist[token]\n",
    "                cntr = index_above_threshold[index]\n",
    "                f1[cntr] = 1\n",
    "                total_score += sentiment_score[cntr]\n",
    "                num_tokens = num_tokens + 1.0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if num_tokens == 0:\n",
    "        avg_score = 0\n",
    "    else:\n",
    "        avg_score = total_score/num_tokens\n",
    "\n",
    "    f1.append(avg_score)\n",
    "\n",
    "    f2 = [ 0 ]*len(sentiment_score_p)\n",
    "    total_score = 0.0\n",
    "    num_tokens = 0.0\n",
    "    for token, pos in phrase_tokens:\n",
    "        try:\n",
    "            index = phraselist[token]\n",
    "            cntr = index_above_threshold_p[index]\n",
    "            f2[cntr] = 1\n",
    "            total_score += sentiment_score_p[cntr]\n",
    "            num_tokens = num_tokens + 1.0\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if num_tokens == 0:\n",
    "        avg_score = 0\n",
    "    else:\n",
    "        avg_score = total_score/num_tokens\n",
    "\n",
    "    f1 = f1 + f2\n",
    "    f1.append(avg_score)\n",
    "    #return [hashlib.md5(''.join(f1).encode()).hexdigest(), avg_score]\n",
    "    #return [hash(''.join(f1)), avg_score]\n",
    "    return np.array(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preInitialize()\n",
    "data = pd.DataFrame()\n",
    "for file in SOURCES:\n",
    "    data = data.append(getDF_all(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10261"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit, transform, fit_transform methods to form vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vectorize():\n",
    "    def fit(self, data):\n",
    "        # print(len(data))\n",
    "        initialize()\n",
    "        # self.pos_s = []\n",
    "        cntr = 0\n",
    "        cntr2 = 0\n",
    "        for index, review in data.iterrows():\n",
    "            #print(review)\n",
    "            count_tokens(review['reviewText'], int(review['overall']))\n",
    "            count_phrase(review['phraseTokens'], int(review['overall']))\n",
    "            # self.pos_s.append(pos)\n",
    "        sentimentScore()\n",
    "\n",
    "    def transform(self, data):\n",
    "        feature_vector_list = []\n",
    "        for index, review in data.iterrows():\n",
    "            feature_vector_list.append(f_vector_p(review['reviewText'], review['phraseTokens']))\n",
    "        return np.array(feature_vector_list)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        # print(len(data))\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#phraselist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples on how to use above classes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorize = Vectorize()\n",
    "feature_vector = vectorize.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10261, 3289)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "targets = data['overall'].values\n",
    "classifier.fit(feature_vector, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  5.,  5., ...,  4.,  4.,  4.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(f_vector_s('Not much to write about here, but it does exactly what it\\'s supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,').reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to perform KFold test with provided classifier for n_folds=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kFoldTest(classifier):\n",
    "    pipeline = Pipeline([\n",
    "            ('vectorizer', Vectorize()),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ])\n",
    "\n",
    "    kFold = KFold(n = len(data), n_folds = 4)\n",
    "    scores = []\n",
    "    confusionMatrix = np.array([[0]*5]*5)\n",
    "    vectorizer = Vectorize()\n",
    "    #classifier = MultinomialNB()\n",
    "    #classifier = BernoulliNB()\n",
    "    #classifier = SVC()\n",
    "    #classifier = RandomForestClassifier()\n",
    "    for train_indices, test_indices in kFold:\n",
    "        train_data = data.iloc[train_indices]\n",
    "        train_y = data.iloc[train_indices]['overall'].values\n",
    "\n",
    "        test_data = data.iloc[test_indices]\n",
    "        test_y = data.iloc[test_indices]['overall'].values\n",
    "\n",
    "        fs = vectorizer.fit_transform(train_data)\n",
    "        classifier.fit(fs, train_y)\n",
    "        # print(fs.shape)\n",
    "        fs = vectorizer.transform(test_data)\n",
    "        # print(fs.shape)\n",
    "        predictions = classifier.predict(fs)\n",
    "\n",
    "        # print(test_y.shape)\n",
    "        # print(predictions.shape)\n",
    "        # print(confusion_matrix(test_y, predictions))\n",
    "        confusionMatrix += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label = 5, average='weighted')\n",
    "        scores.append(score)\n",
    "\n",
    "    print('Total movies classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total movies classified: 10261\n",
      "Score: 0.577732218061\n",
      "Confusion matrix:\n",
      "[[  12    0   14   32  159]\n",
      " [   6    3   13   36  192]\n",
      " [   9    6   24  108  625]\n",
      " [   8    4   35  260 1777]\n",
      " [   6    4   52  504 6372]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total movies classified: 10261\n",
      "Score: 0.607302008853\n",
      "Confusion matrix:\n",
      "[[  15    6   32   33  131]\n",
      " [   2    2   38   59  149]\n",
      " [   1    2   68  202  499]\n",
      " [   1    1   67  421 1594]\n",
      " [   5    3   95  562 6273]]\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total movies classified: 10261\n",
      "Score: 0.577275954265\n",
      "Confusion matrix:\n",
      "[[   7   13   35   27  135]\n",
      " [   6   17   37   38  152]\n",
      " [   5   51   91  152  473]\n",
      " [  15  106  158  377 1428]\n",
      " [  27  249  294  666 5702]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "kFoldTest(BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
