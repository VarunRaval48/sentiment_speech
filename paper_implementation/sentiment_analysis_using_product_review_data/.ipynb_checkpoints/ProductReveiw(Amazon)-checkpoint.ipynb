{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import hashlib\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, average_precision_score, fbeta_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_text = 'reviewText'\n",
    "all_keys = ['reviewText', 'reviewerID', 'reviewTime', 'asin', 'reviewerName', 'overall', 'unixReviewTime', 'summary', 'helpful']\n",
    "req_keys = ['asin', 'reviewText', 'overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordlist = {}\n",
    "wordindex = {}\n",
    "freq_all = []\n",
    "freq = [[] for i in range(5)]\n",
    "i_star = [0, 0, 0, 0, 0]\n",
    "\n",
    "phraselist = {}\n",
    "phraseindex = {}\n",
    "freq_all_p = []\n",
    "freq_p = [[] for i in range(5)]\n",
    "\n",
    "sentiment_pos = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "negation_words = ['not', 'Not']\n",
    "threshold = 10\n",
    "index_above_threshold = {}\n",
    "sentiment_score = []\n",
    "index_above_threshold_p = {}\n",
    "sentiment_score_p = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preInitialize():\n",
    "    wordlist = {}\n",
    "    wordindex = {}\n",
    "    phraselist = {}\n",
    "    phraseindex = {}\n",
    "\n",
    "def initialize():\n",
    "    global wordlist, wordindex, freq_all, freq, i_star, sentiment_score, threshold, index_above_threshold \n",
    "    global phraselist, phraseindex, freq_all_p, freq_p, index_above_threshold_p, sentiment_score_p\n",
    "\n",
    "    freq_all = []\n",
    "    freq = [[] for i in range(5)]\n",
    "\n",
    "    freq_all_p = []\n",
    "    freq_p = [[] for i in range(5)]\n",
    "\n",
    "    i_star = [0, 0, 0, 0, 0]\n",
    "\n",
    "    sentiment_score = []\n",
    "    sentiment_score_p = []\n",
    "    threshold = 10\n",
    "    index_above_threshold = {}\n",
    "    index_above_threshold_p = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(path): \n",
    "    g = gzip.open(path, 'rb') \n",
    "    for l in g:\n",
    "        yield eval(l) \n",
    "\n",
    "def count_tokens(tokens, overall):\n",
    "    i_star[overall-1]+=1\n",
    "\n",
    "    for token, pos in tokens:\n",
    "        try:\n",
    "            index = wordlist[token]\n",
    "            freq_all[index] += 1\n",
    "            freq[overall-1][index] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def count_phrase(phrases, overall):\n",
    "    for token, pos in phrases:\n",
    "        index = phraselist[token]\n",
    "        freq_all_p[index] += 1\n",
    "        freq_p[overall-1][index] += 1\n",
    "\n",
    "def phrase_tagger(tokens):\n",
    "    neg_phrase = []\n",
    "    j = 0\n",
    "    i = 0\n",
    "    l = len(tokens)\n",
    "    while i < (l - 2):\n",
    "        if tokens[i][0] in negation_words:\n",
    "            if tokens[i + 1][1] in adj:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0], 'NOA'))\n",
    "                i+=1\n",
    "            elif tokens[i+1][1] in verb:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0], 'NOV'))\n",
    "                i+=1\n",
    "            elif tokens[i+2][1] in adj:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0] + ' ' + tokens[i + 2][0], 'NOA'))\n",
    "                i+=2\n",
    "            elif tokens[i+2][1] in verb:\n",
    "                neg_phrase.append((tokens[i][0] + ' ' + tokens[i + 1][0] + ' ' + tokens[i + 2][0], 'NOV'))\n",
    "                i+=2\n",
    "        i+=1\n",
    "    return neg_phrase\n",
    "\n",
    "def pos_tagger(sentence):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "def initializeWordPhraselist(tokens, phrases, cntr1, cntr2):\n",
    "    for token, pos in tokens:\n",
    "        if pos in sentiment_pos:\n",
    "            wordlist[token] = cntr1\n",
    "            wordindex[cntr1] = token\n",
    "            cntr1+=1\n",
    "    for token, pos in phrases:\n",
    "        phraselist[token] = cntr2\n",
    "        phraseindex[cntr2] = token\n",
    "        cntr2+=1\n",
    "        \n",
    "    return cntr1, cntr2\n",
    "\n",
    "def getDF_all(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    cntr1 = 0\n",
    "    cntr2 = 0\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        df[i][review_text] = pos_tagger(df[i][review_text])\n",
    "        df[i]['phraseTokens'] = phrase_tagger(df[i][review_text])\n",
    "        cntr1, cntr2 = initializeWordPhraselist(df[i][review_text], df[i]['phraseTokens'], cntr1, cntr2)\n",
    "        i+=1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    cntr1 = 0\n",
    "    cntr2 = 0\n",
    "    for d in parse(path):\n",
    "        df[i] = {key: d[key] for key in req_keys}\n",
    "        # print(cntr, df[i]['overall'])\n",
    "        df[i][review_text] = pos_tagger(df[i][review_text])\n",
    "        df[i]['phraseTokens'] = phrase_tagger(df[i][review_text])\n",
    "        cntr1, cntr2 = initializeWordPhraselist(df[i][review_text], df[i]['phraseTokens'], cntr1, cntr2)\n",
    "        count_tokens(df[i][review_text], int(df[i]['overall']))\n",
    "        count_phrase(df[i]['phraseTokens'], int(df[i]['overall']))\n",
    "        i += 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-a0e90845684f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df = pd.DataFrame({'reviewText': [], 'reviewerID': [], 'reviewTime': [], 'asin': [], 'reviewerName': [], 'overall': [], 'unixReviewTime': [], 'summary': [], 'helpful': []})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSOURCES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-180-afca92c2cdb7>\u001b[0m in \u001b[0;36mgetDF\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mcntr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcntr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializeWordPhraselist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'phraseTokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcntr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcntr2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mcount_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'overall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mcount_phrase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'phraseTokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'overall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-180-afca92c2cdb7>\u001b[0m in \u001b[0;36mcount_phrase\u001b[1;34m(phrases, overall)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphrases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphraselist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mfreq_all_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mfreq_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moverall\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "SOURCES = ['data/reviews_Musical_Instruments_5.json.gz']\n",
    "\n",
    "initialize()\n",
    "df = pd.DataFrame()\n",
    "#df = pd.DataFrame({'reviewText': [], 'reviewerID': [], 'reviewTime': [], 'asin': [], 'reviewerName': [], 'overall': [], 'unixReviewTime': [], 'summary': [], 'helpful': []})\n",
    "for path in SOURCES:\n",
    "    df = df.append(getDF(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1666"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phraselist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16399"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_freq_all = np.array(freq_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16399,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_freq_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_freqs = np.array([freq[0], freq[1], freq[2], freq[3], freq[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16399)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng16SGmfQS1LjDHpJapxBL0mN+/+e4uOAEbOqSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78813e3b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xticks(range(5), ['1-star', '2-star', '3-star', '4-star', '5-star'])\n",
    "plt.yticks()\n",
    "plt.bar(range(5), i_star, align='center', color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentimentScore():\n",
    "    cntr = 0\n",
    "    for word, index in wordlist.items():\n",
    "        if freq_all[index] > threshold:\n",
    "            num = 0\n",
    "            den = 0\n",
    "            for i in range(5):\n",
    "                temp = (i_star[4]/i_star[i]) * freq[i][index]\n",
    "                num += (i+1) * temp\n",
    "                den += temp\n",
    "            #print(num/den)\n",
    "            sentiment_score.append((num/den))\n",
    "            index_above_threshold[index] = cntr\n",
    "            cntr += 1\n",
    "\n",
    "    cntr = 0\n",
    "    for word, index in phraselist.items():\n",
    "        if freq_all_p[index] > threshold:\n",
    "            num = 0\n",
    "            den = 0\n",
    "            for i in range(5):\n",
    "                temp = (i_star[4]/i_star[i]) * freq_p[i][index]\n",
    "                num += (i+1) * temp\n",
    "                den += temp\n",
    "            #print(num/den)\n",
    "            sentiment_score_p.append((num/den))\n",
    "            index_above_threshold_p[index] = cntr\n",
    "            cntr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentimentScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2989"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sen_tokens = len(sentiment_score)\n",
    "len_sen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_score_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_vector_s(sentence):\n",
    "    pos_tokens = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    phrase_tokens = phrase_tagger(pos_tokens)\n",
    "    return f_vector_p(pos_tokens, phrase_tokens)\n",
    "\n",
    "def f_vector_p(pos_tokens, phrase_tokens):\n",
    "    f1 = [ 0 ]*len(sentiment_score)\n",
    "    total_score = 0.0\n",
    "    num_tokens = 0.0\n",
    "    for token, pos in pos_tokens:\n",
    "        if pos in sentiment_pos:\n",
    "            try:\n",
    "                index = wordlist[token]\n",
    "                cntr = index_above_threshold[index]\n",
    "                f1[cntr] = 1\n",
    "                total_score += sentiment_score[cntr]\n",
    "                num_tokens = num_tokens + 1.0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if num_tokens == 0:\n",
    "        avg_score = 0\n",
    "    else:\n",
    "        avg_score = total_score/num_tokens\n",
    "\n",
    "    f1.append(avg_score)\n",
    "\n",
    "    f2 = [ 0 ]*len(sentiment_score_p)\n",
    "    total_score = 0.0\n",
    "    num_tokens = 0.0\n",
    "    for token, pos in phrase_tokens:\n",
    "        try:\n",
    "            index = phraselist[token]\n",
    "            cntr = index_above_threshold_p[index]\n",
    "            f2[cntr] = 1\n",
    "            total_score += sentiment_score_p[cntr]\n",
    "            num_tokens = num_tokens + 1.0\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if num_tokens == 0:\n",
    "        avg_score = 0\n",
    "    else:\n",
    "        avg_score = total_score/num_tokens\n",
    "\n",
    "    f1 = f1 + f2\n",
    "    f1.append(avg_score)\n",
    "    #return [hashlib.md5(''.join(f1).encode()).hexdigest(), avg_score]\n",
    "    #return [hash(''.join(f1)), avg_score]\n",
    "    return np.array(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for file in SOURCES:\n",
    "    data = data.append(getDF_all(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10261"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vectorize():\n",
    "    def fit(self, data):\n",
    "        print(len(data))\n",
    "        initialize()\n",
    "        self.data = data\n",
    "        # self.pos_s = []\n",
    "        cntr = 0\n",
    "        cntr2 = 0\n",
    "        for index, review in data.iterrows():\n",
    "            #print(review)\n",
    "            cntr = count_tokens(review['reviewText'], int(review['overall']), cntr)\n",
    "            cntr2 = count_phrase(review['phraseTokens'], int(review['overall']), cntr2)\n",
    "            # self.pos_s.append(pos)\n",
    "        sentimentScore()\n",
    "\n",
    "    def transform(self):\n",
    "        feature_vector_list = []\n",
    "        for index, review in self.data.iterrows():\n",
    "            feature_vector_list.append(f_vector_p(review['reviewText'], review['phraseTokens']))\n",
    "        return np.array(feature_vector_list)        \n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        print(len(data))\n",
    "        self.fit(data)\n",
    "        return self.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#phraselist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10261\n",
      "10261\n"
     ]
    }
   ],
   "source": [
    "vectorize = Vectorize()\n",
    "feature_vector = vectorize.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10261, 3048)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "targets = data['overall'].values\n",
    "classifier.fit(feature_vector, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  5.,  5., ...,  4.,  4.,  4.])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(f_vector_s('Not much to write about here, but it does exactly what it\\'s supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,').reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kFoldTest(classifier):\n",
    "    pipeline = Pipeline([\n",
    "            ('vectorizer', Vectorize()),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ])\n",
    "\n",
    "    kFold = KFold(n = len(data), n_folds = 4)\n",
    "    scores = []\n",
    "    confusionMatrix = np.array([[0]*5]*5)\n",
    "    vectorizer = Vectorize()\n",
    "    #classifier = MultinomialNB()\n",
    "    #classifier = BernoulliNB()\n",
    "    #classifier = SVC()\n",
    "    #classifier = RandomForestClassifier()\n",
    "    for train_indices, test_indices in kFold:\n",
    "        train_data = data.iloc[train_indices]\n",
    "        train_y = data.iloc[train_indices]['overall'].values\n",
    "\n",
    "        test_data = data.iloc[test_indices]\n",
    "        test_y = data.iloc[test_indices]['overall'].values\n",
    "\n",
    "        fs = vectorizer.fit_transform(train_data)\n",
    "        classifier.fit(fs, train_y)\n",
    "        print(fs.shape)\n",
    "        fs = vectorizer.fit_transform(test_data)\n",
    "        print(fs.shape)\n",
    "        predictions = classifier.predict(fs)\n",
    "\n",
    "        print(test_y.shape)\n",
    "        print(predictions.shape)\n",
    "        print(confusion_matrix(test_y, predictions))\n",
    "        confusionMatrix += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label = 5)\n",
    "        scores.append(score)\n",
    "\n",
    "    print('Total movies classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7695\n",
      "7695\n",
      "(7695, 2606)\n",
      "2566\n",
      "2566\n",
      "(2566, 1073)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must  match the input. Model n_features is 2606 and  input n_features is 1073 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-6eec49bc5f91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkFoldTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-169-905ca92802d2>\u001b[0m in \u001b[0;36mkFoldTest\u001b[1;34m(classifier)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \"\"\"\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \"\"\"\n\u001b[0;32m    536\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    317\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    374\u001b[0m                              \u001b[1;34m\" match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                              \u001b[1;34m\" input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must  match the input. Model n_features is 2606 and  input n_features is 1073 "
     ]
    }
   ],
   "source": [
    "kFoldTest(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
